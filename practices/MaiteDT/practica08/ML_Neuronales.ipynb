{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "id": "1X7V4TCEx0VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISwIkXesaeT4",
        "outputId": "89ec96c0-99ef-4b24-d1a0-21a0db96bc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CORPORA_DIR = \"drive/MyDrive/Quijote.txt\"\n",
        "\n",
        "corpus_sent = []\n",
        "with open(CORPORA_DIR, 'r') as file:\n",
        "    for line in file:\n",
        "        corpus_sent.append(line)"
      ],
      "metadata": {
        "id": "4lDF7AFIaMKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sent[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzYTQvsLjrUO",
        "outputId": "f99b8800-67f0-4ccb-d420-a39a66f302fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffThe Project Gutenberg eBook of Don Quijote\\n',\n",
              " '    \\n',\n",
              " 'This ebook is for the use of anyone anywhere in the United States and\\n',\n",
              " 'most other parts of the world at no cost and with almost no restrictions\\n',\n",
              " 'whatsoever. You may copy it, give it away or re-use it under the terms\\n',\n",
              " 'of the Project Gutenberg License included with this ebook or online\\n',\n",
              " 'at www.gutenberg.org. If you are not located in the United States,\\n',\n",
              " 'you will have to check the laws of the country where you are located\\n',\n",
              " 'before using this eBook.\\n',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocesamiento\n",
        "Queremos quitar la mayor parte de líneas en blanco y el header que está en inglés"
      ],
      "metadata": {
        "id": "J5-2YRcZ1XXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus(sent_corpus : list[str]):\n",
        "  '''\n",
        "  Quitamos los espacios en blanco y líneas que solo es \\n\n",
        "  '''\n",
        "  clean = []\n",
        "  for sent in sent_corpus:\n",
        "    if ((sent != '\\n') & (sent != '  \\n')):\n",
        "      clean.append(sent)\n",
        "\n",
        "  return clean"
      ],
      "metadata": {
        "id": "8rud9Ee3y1xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = clean_corpus(corpus_sent)\n",
        "corpus[:16]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNzRZXK4zN8N",
        "outputId": "42d16de8-d99f-42b1-a5d3-fe8d5aeec65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffThe Project Gutenberg eBook of Don Quijote\\n',\n",
              " '    \\n',\n",
              " 'This ebook is for the use of anyone anywhere in the United States and\\n',\n",
              " 'most other parts of the world at no cost and with almost no restrictions\\n',\n",
              " 'whatsoever. You may copy it, give it away or re-use it under the terms\\n',\n",
              " 'of the Project Gutenberg License included with this ebook or online\\n',\n",
              " 'at www.gutenberg.org. If you are not located in the United States,\\n',\n",
              " 'you will have to check the laws of the country where you are located\\n',\n",
              " 'before using this eBook.\\n',\n",
              " 'Title: Don Quijote\\n',\n",
              " 'Author: Miguel de Cervantes Saavedra\\n',\n",
              " 'Release date: December 1, 1999 [eBook #2000]\\n',\n",
              " '                Most recently updated: January 17, 2021\\n',\n",
              " 'Language: Spanish\\n',\n",
              " 'Credits: an anonymous Project Gutenberg volunteer and Joaquin Cuenca Abela\\n',\n",
              " '*** START OF THE PROJECT GUTENBERG EBOOK DON QUIJOTE ***\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = list(range(16))\n",
        "quijote_sents = np.delete(corpus, indexes)\n",
        "quijote_sents[:10]\n",
        "##Quitamos el header del documento para trabajar sólo con el texto del quijote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whbwAe1w0NsP",
        "outputId": "d2737a31-d4b8-4240-b96b-3c13c97f2b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['El ingenioso hidalgo don Quijote de la Mancha\\n',\n",
              "       'por Miguel de Cervantes Saavedra\\n',\n",
              "       'El ingenioso hidalgo don Quijote de la Mancha\\n', 'Tasa\\n',\n",
              "       'Testimonio de las erratas\\n', 'El Rey\\n', 'Al Duque de Béjar\\n',\n",
              "       'Prólogo\\n', 'Al libro de don Quijote de la Mancha\\n',\n",
              "       'Que trata de la condición y ejercicio del famoso\\n'], dtype='<U79')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_to_words(sent_corpus : list[str]) -> list[str]:\n",
        "  new_corpus = []\n",
        "  for sent in sent_corpus:\n",
        "    new_corpus.append([word.strip() for word in sent.split(' ')])\n",
        "\n",
        "  return new_corpus"
      ],
      "metadata": {
        "id": "NuRJkZBDssKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quijote_corpus = sentences_to_words(quijote_sents)"
      ],
      "metadata": {
        "id": "ZS-ylan9tsan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quijote_corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsXmC4kU08bh",
        "outputId": "585d38de-c6bd-41df-fbb0-f7c5774a8fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['El', 'ingenioso', 'hidalgo', 'don', 'Quijote', 'de', 'la', 'Mancha'],\n",
              " ['por', 'Miguel', 'de', 'Cervantes', 'Saavedra'],\n",
              " ['El', 'ingenioso', 'hidalgo', 'don', 'Quijote', 'de', 'la', 'Mancha'],\n",
              " ['Tasa'],\n",
              " ['Testimonio', 'de', 'las', 'erratas'],\n",
              " ['El', 'Rey'],\n",
              " ['Al', 'Duque', 'de', 'Béjar'],\n",
              " ['Prólogo'],\n",
              " ['Al', 'libro', 'de', 'don', 'Quijote', 'de', 'la', 'Mancha'],\n",
              " ['Que', 'trata', 'de', 'la', 'condición', 'y', 'ejercicio', 'del', 'famoso']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, tenemos que agregar tokens EOS, BS y normalizar a minúsculas"
      ],
      "metadata": {
        "id": "tn6Cl-1y1qDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess_corpus(corpus: list[str]) -> list[str]:\n",
        "    \"\"\"Función de preprocesamiento\n",
        "\n",
        "    Agrega tokens de inicio y fin, normaliza todo a minusculas\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sent in corpus:\n",
        "        result = [word.lower() for word in sent]\n",
        "        # Al final de la oración\n",
        "        result.append(\"<EOS>\")\n",
        "        result.insert(0, \"<BOS>\")\n",
        "        preprocessed_corpus.append(result)\n",
        "    return preprocessed_corpus"
      ],
      "metadata": {
        "id": "99YWB-xUmRYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_freqs(corpus: list[list[str]]):\n",
        "    words_freqs = {}\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            words_freqs[word] = words_freqs.get(word, 0) + 1\n",
        "    return words_freqs"
      ],
      "metadata": {
        "id": "7EoMeauHmWVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_LABEL = \"<UNK>\"\n",
        "def get_words_indexes(words_freqs: dict) -> dict:\n",
        "    result = {}\n",
        "    for idx, word in enumerate(words_freqs.keys()):\n",
        "        # Happax legomena happends\n",
        "        if words_freqs[word] == 1:\n",
        "            # Temp index for unknowns\n",
        "            result[UNK_LABEL] = len(words_freqs)\n",
        "        else:\n",
        "            result[word] = idx\n",
        "\n",
        "    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}"
      ],
      "metadata": {
        "id": "GRk-VoS_mZ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quijote_processed = preprocess_corpus(quijote_corpus)"
      ],
      "metadata": {
        "id": "CgsIe7LGmdS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quijote_processed[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13z62EeYnglA",
        "outputId": "a89f2b2c-1bcb-4647-8b8a-43aaac9134ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<BOS>',\n",
              "  'el',\n",
              "  'ingenioso',\n",
              "  'hidalgo',\n",
              "  'don',\n",
              "  'quijote',\n",
              "  'de',\n",
              "  'la',\n",
              "  'mancha',\n",
              "  '<EOS>'],\n",
              " ['<BOS>', 'por', 'miguel', 'de', 'cervantes', 'saavedra', '<EOS>'],\n",
              " ['<BOS>',\n",
              "  'el',\n",
              "  'ingenioso',\n",
              "  'hidalgo',\n",
              "  'don',\n",
              "  'quijote',\n",
              "  'de',\n",
              "  'la',\n",
              "  'mancha',\n",
              "  '<EOS>'],\n",
              " ['<BOS>', 'tasa', '<EOS>'],\n",
              " ['<BOS>', 'testimonio', 'de', 'las', 'erratas', '<EOS>'],\n",
              " ['<BOS>', 'el', 'rey', '<EOS>'],\n",
              " ['<BOS>', 'al', 'duque', 'de', 'béjar', '<EOS>'],\n",
              " ['<BOS>', 'prólogo', '<EOS>'],\n",
              " ['<BOS>',\n",
              "  'al',\n",
              "  'libro',\n",
              "  'de',\n",
              "  'don',\n",
              "  'quijote',\n",
              "  'de',\n",
              "  'la',\n",
              "  'mancha',\n",
              "  '<EOS>'],\n",
              " ['<BOS>',\n",
              "  'que',\n",
              "  'trata',\n",
              "  'de',\n",
              "  'la',\n",
              "  'condición',\n",
              "  'y',\n",
              "  'ejercicio',\n",
              "  'del',\n",
              "  'famoso',\n",
              "  '<EOS>']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(quijote_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NUpr5FHmloS",
        "outputId": "67832eb3-5fe7-49a9-f203-893bb07d4893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32229"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_freqs = get_words_freqs(quijote_processed)"
      ],
      "metadata": {
        "id": "GbtaVAlJmvl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFjaR4Comsro",
        "outputId": "441f8e89-a6b2-41bd-af82-9d8899fe4f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38106"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_freqs[\"el\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijTw1oWY4Xc4",
        "outputId": "1c0a2eba-f41f-4e56-a9cf-ad180bc7a446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8254"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for word, freq in words_freqs.items():\n",
        "    if freq == 1 and count <= 10:\n",
        "        print(word, freq)\n",
        "        count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgii1EpX2aPV",
        "outputId": "0ee6fdda-4364-4229-f135-de4e9ac3293e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saavedra 1\n",
            "papel; 1\n",
            "conste, 1\n",
            "deciembre 1\n",
            "andrada. 1\n",
            "correcto, 1\n",
            "fee. 1\n",
            "diciembre 1\n",
            "1604 1\n",
            "pedistes 1\n",
            "previlegio 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes, index_to_word = get_words_indexes(words_freqs)"
      ],
      "metadata": {
        "id": "yKK-Yr6S36da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes[\"el\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6BHaavt39C7",
        "outputId": "412c8549-dc75-41a7-86fb-cf8898ac3629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9LLXGaZh4dVK",
        "outputId": "58b7ad88-ca03-4bce-bce0-7c0401ac5bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'el'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YsQi6yp4iLT",
        "outputId": "dc3f1d38-bd01-404c-d0f6-013e89eca886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16036"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYbxeVAI4jIA",
        "outputId": "9f3a2584-49d3-40ea-df6a-7e5d27419174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16036"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_id(words_indexes: dict, word: str) -> int:\n",
        "    unk_word_id = words_indexes[UNK_LABEL]\n",
        "    return words_indexes.get(word, unk_word_id)"
      ],
      "metadata": {
        "id": "tWeEz5z64nzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtenemos trigramas"
      ],
      "metadata": {
        "id": "oJ5v4XLs4tZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ],
      "metadata": {
        "id": "JLjg-6XU4vtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(corpus: list[list[str]], words_indexes: dict, n: int) -> tuple[list, list]:\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for sent in corpus:\n",
        "        n_grams = ngrams(sent, n)\n",
        "        for w1, w2, w3 in n_grams:\n",
        "            x_train.append([get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)])\n",
        "            y_train.append([get_word_id(words_indexes, w3)])\n",
        "    return x_train, y_train"
      ],
      "metadata": {
        "id": "JcPONC-E5JxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup de parametros\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = 2\n",
        "BATCH_SIZE = 256\n",
        "H = 100\n",
        "torch.manual_seed(19)\n",
        "# Tamaño del Vocabulario\n",
        "V = len(words_indexes)"
      ],
      "metadata": {
        "id": "ZRzEX-UL402X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_train_test_data(quijote_processed, words_indexes, n=3)"
      ],
      "metadata": {
        "id": "eaSsDnnd44sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "# partimos los datos de entrada en batches\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "9M_lox_r5kEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Neural Network Model\n",
        "class TrigramModel(nn.Module):\n",
        "    \"\"\"Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # x': concatenation of x1 and x2 embeddings   -->\n",
        "        #self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada. view() les cambia el tamaño para concatenarlos\n",
        "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
        "        # h: tanh(W_1.x' + b)  -->\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # W_2.h                 -->\n",
        "        out = self.linear2(out)\n",
        "        # log_softmax(W_2.h)      -->\n",
        "        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "mvDi-Qkj5pdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento"
      ],
      "metadata": {
        "id": "Ig_axxT45sUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pérdida. Negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "#Otras opciones de función de pérdida (tendrían que usar softmax sin log):\n",
        "#nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# 2. Instanciar el modelo\n",
        "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "\n",
        "# 3. Optimización. ADAM optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "#Otras opciones de optimizador:\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
        "# En la práctica sólo correremos una epoch por restricciones de recursos\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    st = time.time()\n",
        "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
        "    for it, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2]\n",
        "        target_tensor = data_tensor[:,2]\n",
        "\n",
        "        model.zero_grad() #reinicializar los gradientes\n",
        "        #FORWARD:\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        #BACKWARD:\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 500 == 0:\n",
        "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
        "            st = time.time()\n",
        "            #barch_size x len(vocab)\n",
        "\n",
        "    # saving model\n",
        "    model_path = \"drive/MyDrive/\" + 'model_{}.dat'.format(epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved for epoch={epoch} at {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr4X8o1v5r7b",
        "outputId": "62d778a5-e3d6-4404-dd94-bf46a7dd4073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training model Epoch: 0 ---\n",
            "Training Iteration 0 of epoch 0 complete. Loss: 9.738958358764648; Time taken (s): 0.4195072650909424\n",
            "Training Iteration 500 of epoch 0 complete. Loss: 5.58427619934082; Time taken (s): 70.41594457626343\n",
            "Training Iteration 1000 of epoch 0 complete. Loss: 5.266488075256348; Time taken (s): 67.49588203430176\n",
            "Training Iteration 1500 of epoch 0 complete. Loss: 5.469616889953613; Time taken (s): 69.2159628868103\n",
            "Model saved for epoch=0 at drive/MyDrive/model_0.dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAnVfMRz6rs8",
        "outputId": "e9b77405-7742-4a30-df69-d98437c88f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrigramModel(\n",
              "  (embeddings): Embedding(16036, 200)\n",
              "  (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
              "  (linear2): Linear(in_features=100, out_features=16036, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(path: str) -> TrigramModel:\n",
        "    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "    model_loaded.load_state_dict(torch.load(path))\n",
        "    model_loaded.eval()\n",
        "    return model_loaded"
      ],
      "metadata": {
        "id": "KN0zyk0i6u29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"drive/MyDrive/model_0.dat\""
      ],
      "metadata": {
        "id": "4aBSosCH6y_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(PATH)\n",
        "W1 = \"<BOS>\"\n",
        "W2 = \"my\"\n",
        "\n",
        "IDX1 = get_word_id(words_indexes, W1)\n",
        "IDX2 = get_word_id(words_indexes, W2)\n",
        "\n",
        "#Obtenemos Log probabidades p(W3|W2,W1)\n",
        "probs = model(torch.tensor([[IDX1,  IDX2]])).detach().tolist()"
      ],
      "metadata": {
        "id": "E4ZUamum61wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(probs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxf4n_37-xHQ",
        "outputId": "6deda13e-50ed-4d90-ef5c-f92a5e6d132f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16036"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos diccionario con {idx: logprob}\n",
        "model_probs = {}\n",
        "for idx, p in enumerate(probs[0]):\n",
        "  model_probs[idx] = p\n",
        "\n",
        "# Sort:\n",
        "model_probs_sorted = sorted(((prob, idx) for idx, prob in model_probs.items()), reverse=True)\n",
        "\n",
        "# Printing word  and prob (retrieving the idx):\n",
        "topcandidates = 0\n",
        "for prob, idx in model_probs_sorted:\n",
        "  #Retrieve the word associated with that idx\n",
        "  word = index_to_word[idx]\n",
        "  print(idx, word, prob)\n",
        "\n",
        "  topcandidates += 1\n",
        "\n",
        "  if topcandidates > 100:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0LRU-hp-08V",
        "outputId": "13df43ce-854d-42cb-ad6d-5a90d6b334e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15787 to -3.857161045074463\n",
            "12331 in -3.87473201751709\n",
            "15828 or -4.590223789215088\n",
            "15747 of -4.617570400238037\n",
            "15801 you -4.634593963623047\n",
            "15748 the -4.770502090454102\n",
            "15800 if -5.2651519775390625\n",
            "15775 and -5.4163360595703125\n",
            "15818 such -5.532860279083252\n",
            "15619 llevadme -6.096963882446289\n",
            "13102 ¡cuerpo -6.125380039215088\n",
            "15003 date -6.126634120941162\n",
            "15906 format -6.1752610206604\n",
            "15949 marked -6.261355876922607\n",
            "15720 data, -6.407069206237793\n",
            "15545 atropellando -6.4084672927856445\n",
            "15767 owns -6.446159839630127\n",
            "15758 from -6.4531331062316895\n",
            "15480 ricote -6.4932074546813965\n",
            "13684 déjenme -6.506566524505615\n",
            "15781 set -6.545663833618164\n",
            "13890 nueces, -6.545879364013672\n",
            "15568 pastoras -6.577900409698486\n",
            "14243 viniéndosele -6.585544586181641\n",
            "14112 justas, -6.604029655456543\n",
            "15347 parecíale -6.609562873840332\n",
            "15766 that -6.615713596343994\n",
            "15858 your -6.637983798980713\n",
            "14298 callaron -6.638277053833008\n",
            "15259 tocaron -6.662238597869873\n",
            "14992 ¡par -6.66354513168335\n",
            "14460 volví -6.685971736907959\n",
            "15916 concerning -6.739814758300781\n",
            "13189 cantó -6.740317344665527\n",
            "14961 tembló -6.746699810028076\n",
            "12355 atendiendo -6.752817630767822\n",
            "5442 cesó -6.753262519836426\n",
            "15843 all -6.764737129211426\n",
            "10878 amiga, -6.807456016540527\n",
            "14209 oveja -6.817197322845459\n",
            "14174 ved -6.821671485900879\n",
            "15846 at -6.822169780731201\n",
            "12191 ¡aquí -6.82503604888916\n",
            "15824 full -6.836702346801758\n",
            "13640 puercos, -6.846128940582275\n",
            "469 altisidora -6.8594536781311035\n",
            "13518 aderezado -6.8599066734313965\n",
            "15099 depositar -6.860328674316406\n",
            "14208 viva, -6.878558158874512\n",
            "15335 ¡medrados -6.882542133331299\n",
            "15749 project -6.884918212890625\n",
            "15370 ¿digo -6.9055657386779785\n",
            "14908 gobernador? -6.919652938842773\n",
            "13232 carrasco, -6.9283952713012695\n",
            "14721 ¿ahora, -6.944068908691406\n",
            "15809 including -6.946181297302246\n",
            "15447 fingida -6.9498209953308105\n",
            "15332 tenemos! -6.955409526824951\n",
            "13821 refranes, -6.964861869812012\n",
            "10626 abrazóle -6.967144966125488\n",
            "6096 sucedió, -6.968111515045166\n",
            "13891 secas -6.97134256362915\n",
            "15783 terms -6.994847774505615\n",
            "13336 embista -7.006113529205322\n",
            "14723 ¿ahora -7.008420467376709\n",
            "13918 campea -7.009483814239502\n",
            "14476 lléveme -7.02427864074707\n",
            "14643 morisma, -7.030834674835205\n",
            "14258 habilidades -7.039515495300293\n",
            "15079 condesa. -7.042712688446045\n",
            "7011 ¡ea, -7.044318199157715\n",
            "12871 alhombra, -7.051770210266113\n",
            "15314 estimadas -7.061783313751221\n",
            "14959 sonaron -7.075076580047607\n",
            "15277 hele -7.075390338897705\n",
            "14617 \"a -7.080178260803223\n",
            "12612 use -7.082815647125244\n",
            "13412 ¡no -7.085587978363037\n",
            "15213 entera, -7.088057041168213\n",
            "15725 tarfe, -7.097154140472412\n",
            "6782 trujéronle -7.099614143371582\n",
            "14085 apeóse -7.107698917388916\n",
            "15753 editions -7.108981132507324\n",
            "15894 we -7.122566223144531\n",
            "13021 quisieras -7.130547523498535\n",
            "15770 these -7.134490489959717\n",
            "15438 consuela -7.141549587249756\n",
            "13216 etcétera. -7.144509315490723\n",
            "14704 ¡tan -7.149087905883789\n",
            "14756 puntos, -7.157742023468018\n",
            "14111 mesa; -7.172487735748291\n",
            "15284 ¡justicia -7.188400745391846\n",
            "12927 pisando -7.204348564147949\n",
            "15866 entity -7.207939147949219\n",
            "4795 ¡qué -7.208046913146973\n",
            "11975 calló, -7.208357334136963\n",
            "14315 gansos -7.212894916534424\n",
            "15212 tiesas -7.214471817016602\n",
            "14817 duquesa—, -7.221396446228027\n",
            "13761 ¡y -7.221691131591797\n",
            "13644 metiendo -7.22546911239624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word.get(model_probs_sorted[0][0])"
      ],
      "metadata": {
        "id": "DMqYVDSO-3b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liga al [modelo](https://drive.google.com/file/d/1-24lp3GYh-HqMuhRpAYrla-F3V03o0Sj/view?usp=drive_link) en Drive"
      ],
      "metadata": {
        "id": "zgceWPaB1SFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generación de lenguaje"
      ],
      "metadata": {
        "id": "_5cKFweG_B4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_likely_words(model: TrigramModel, context: str, words_indexes: dict, index_to_word: dict, top_count: int=10) -> list[tuple]:\n",
        "    model_probs = {}\n",
        "    words = context.split()\n",
        "    idx_word_1 = get_word_id(words_indexes, words[0])\n",
        "    idx_word_2 = get_word_id(words_indexes, words[1])\n",
        "    probs = model(torch.tensor([[idx_word_1, idx_word_2]])).detach().tolist()\n",
        "\n",
        "    for idx, p in enumerate(probs[0]):\n",
        "        model_probs[idx] = p\n",
        "\n",
        "    # Strategy: Sort and get top-K words to generate text\n",
        "    return sorted(((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True)[:top_count]"
      ],
      "metadata": {
        "id": "N9rH-Sf3_E3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"fuimos a\"\n",
        "get_likely_words(model, sentence, words_indexes, index_to_word, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5EPCNMP_MvQ",
        "outputId": "a0a7a65a-df54-4dae-deb5-a4d4dc5257a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-4.023273944854736, 'barcelona,'),\n",
              " (-4.595625877380371, 'verle,'),\n",
              " (-4.597508907318115, 'ti')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"con el\"\n",
        "get_likely_words(model, sentence, words_indexes, index_to_word, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRVXzMW3_Pgr",
        "outputId": "24cc6f98-b18d-4775-9071-91273a52fd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-4.023273944854736, 'barcelona,'),\n",
              " (-4.595625877380371, 'verle,'),\n",
              " (-4.597508907318115, 'ti')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2 = \"vimos un\"\n",
        "get_likely_words(model, sentence, words_indexes, index_to_word, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBSJF6FS_TZU",
        "outputId": "21577f4c-00f5-4cb1-f2a2-c7517624094a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-4.023273944854736, 'barcelona,'),\n",
              " (-4.595625877380371, 'verle,'),\n",
              " (-4.597508907318115, 'ti')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def get_next_word(words: list[tuple[float, str]]) -> str:\n",
        "    # From a top-K list of words get a random word\n",
        "    return words[randint(0, len(words)-1)][1]"
      ],
      "metadata": {
        "id": "Q1UvFr00_Zlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_next_word(get_likely_words(model, sentence2, words_indexes, index_to_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f2B-Ze13_c6F",
        "outputId": "b935dfab-11ef-4e30-9fd8-eb345b4c1db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'buen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 30\n",
        "TOP_COUNT = 10\n",
        "def generate_text(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:\n",
        "    next_word = get_next_word(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))\n",
        "    print(next_word, end=\" \")\n",
        "    tokens_count += 1\n",
        "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
        "        return\n",
        "    generate_text(model, history.split()[1]+ \" \" + next_word, words_indexes, index_to_word, tokens_count)"
      ],
      "metadata": {
        "id": "fgYdKLJN_mWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"<BOS> fue\"\n",
        "print(sent, end=\" \")\n",
        "generate_text(model, sent, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyK5DndN_15w",
        "outputId": "46fabde7-b3eb-451b-ca78-b26e2e8025e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> fue gracioso, con los duques <UNK> of puestas, manera: <EOS> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"<BOS> en\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x63ZJoCp_pAX",
        "outputId": "dcf23cd8-b16a-4a94-812a-7a1fcc64679d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> en ala, <UNK> ricote you may mía! que <UNK> yendo pues, <EOS> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"<BOS> no\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOOWK_UVAAIs",
        "outputId": "ad53612c-72ff-43af-a385-6b3e4b9c5b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> no confesáis posible que <EOS> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"ese fue\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwQ8ge8IA62Q",
        "outputId": "3f1167f7-4f4a-4389-afa6-ed720cb35fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ese fue que en la lengua figura, que, pues me ha querido de lo de <UNK> in félix mismo, y en el aposento don quijote; que, puesto de que yo pienso haber "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embeddings"
      ],
      "metadata": {
        "id": "Lfi-vRNA4G-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = input(\">> \")\n",
        "words_tensor = torch.LongTensor([get_word_id(words_indexes, word)])\n",
        "word_embed = model.embeddings(words_tensor)\n",
        "print(f\"embbeding (dim={len(word_embed[0])}) vec for word={word}\")\n",
        "word_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBJnJkVm5LKD",
        "outputId": "acaaea43-5b3f-48f5-b76b-de9d09b9edba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> dijistes\n",
            "embbeding (dim=200) vec for word=dijistes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7049, -0.8217, -1.4046, -0.4242, -0.3843,  1.2171,  2.0635,  1.0503,\n",
              "         -1.0077, -0.5408,  1.3352,  0.1515,  1.3394, -0.6162,  0.0187,  0.4192,\n",
              "          0.8145,  0.7169, -0.4935, -1.0501, -0.5550,  0.1048,  2.4219,  0.6141,\n",
              "          0.1410,  0.6613,  0.1001,  0.1726, -0.0041,  0.1390, -0.0734, -1.3170,\n",
              "         -0.1972, -0.7047, -1.1203,  1.3664, -0.2810, -0.0114,  0.6651,  0.4549,\n",
              "         -0.8410, -0.4305,  0.3905, -0.3211, -0.7807, -0.7998,  0.7703,  0.6948,\n",
              "         -0.1618,  0.9177,  1.1669, -0.7677, -0.4478,  0.6371,  1.3727,  0.7251,\n",
              "          0.8919,  1.0914, -1.0731, -0.0081, -0.7819, -0.3320,  0.5162, -1.1513,\n",
              "          1.7838,  0.0222,  0.4287,  1.0062,  0.2067, -0.5762, -0.0311,  0.8352,\n",
              "          1.1277,  0.6921,  0.3941, -0.4135,  1.6018,  1.3549,  0.0283, -0.4879,\n",
              "         -0.1269, -0.2917, -0.2915, -0.4840, -0.3493, -0.8144,  1.2907,  1.5847,\n",
              "          0.8793,  1.0309,  0.6827,  0.8596, -0.8594, -0.6392, -0.9663,  0.9392,\n",
              "         -0.4985, -0.1583,  0.4308,  1.3220,  0.0336,  1.6719,  1.2336, -0.2965,\n",
              "         -0.6465,  0.0909, -0.1896,  0.8493, -0.5964,  0.1264, -2.0205, -1.0836,\n",
              "          1.0329, -0.7199, -1.1785, -0.3173, -0.7130,  0.2897,  1.3473, -1.1173,\n",
              "         -0.0354, -1.0641, -0.2071,  0.9393, -0.1495, -0.3382, -0.6258, -0.7108,\n",
              "          0.5729, -0.2094, -0.2412,  0.0922,  0.9066,  1.2573, -0.3868, -0.3724,\n",
              "          0.3894, -1.4989, -1.5475,  0.7755,  0.3028, -1.2310,  0.4353, -0.1411,\n",
              "         -0.8004, -0.7613, -0.5377,  0.3458, -0.7720, -1.0882,  0.0166, -0.7009,\n",
              "          0.1818, -0.6755,  0.0756,  0.2473,  0.6839,  0.1748, -1.9051, -0.8949,\n",
              "          0.7796, -0.1670, -0.8492,  0.1722, -0.4268, -1.3645, -0.2726, -0.9704,\n",
              "          0.3781,  0.8190, -0.0129,  1.1125, -0.2184,  1.1901,  0.6564,  0.4397,\n",
              "          0.1399,  0.2884,  1.2208, -0.5406,  0.5341,  0.9874, -1.3367, -0.8821,\n",
              "          0.9373,  0.1711, -0.2915, -1.0773,  0.8377, -0.5105,  0.6661, -0.6148,\n",
              "          0.3491, -0.0959, -1.0909,  1.0390, -0.3486,  0.2108, -0.9227, -1.0288]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Obtenemos embedding para una lista de palabras\n",
        "words = [\"el\", \"avanzó\", \"día\", \"ayer\"]\n",
        "for word in words:\n",
        "    base_word_embed = model.embeddings(torch.LongTensor([get_word_id(words_indexes, word)])).detach().numpy()\n",
        "\n",
        "    # Calcula la similitud del coseno con todas las otras palabras\n",
        "    word_sims = {}\n",
        "    for other_word in words_indexes.keys():\n",
        "        if word == other_word:\n",
        "            continue\n",
        "        other_word_embed = model.embeddings(torch.LongTensor([get_word_id(words_indexes, other_word)])).detach().numpy()\n",
        "        word_sims[other_word] = cosine_similarity(base_word_embed, other_word_embed)\n",
        "\n",
        "    # Imprime las 10 palabras más similares\n",
        "    print(\"\\nBASE WORD =\", word)\n",
        "    for word, sim in sorted(word_sims.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
        "        print(f\"{word}: {sim[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvxGgDTQ6hXE",
        "outputId": "8ea934e3-8487-40df-f6ee-e3da2df959ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BASE WORD = el\n",
            "desvalijando: 0.28377223014831543\n",
            "pequeño: 0.2815472483634949\n",
            "sueños: 0.2502957284450531\n",
            "tirador: 0.24889090657234192\n",
            "actores: 0.24482297897338867\n",
            "jaula: 0.24098268151283264\n",
            "vivaldo: 0.23634451627731323\n",
            "son.: 0.23514550924301147\n",
            "llámase: 0.23087884485721588\n",
            "esposo: 0.2296067178249359\n",
            "\n",
            "BASE WORD = avanzó\n",
            "<UNK>: 1.0\n",
            "antojo: 0.25950002670288086\n",
            "quebrantado,: 0.24768158793449402\n",
            "balcón,: 0.237320676445961\n",
            "personas:: 0.237098827958107\n",
            "¿esta: 0.23624125123023987\n",
            "aventura,: 0.2350798398256302\n",
            "hombro: 0.23226578533649445\n",
            "ciudad;: 0.22861920297145844\n",
            "gana,: 0.22807221114635468\n",
            "\n",
            "BASE WORD = día\n",
            "conocer: 0.3051227629184723\n",
            "letu-,: 0.26607033610343933\n",
            "manda.: 0.25617092847824097\n",
            "acabar.: 0.2509163022041321\n",
            "castellana,: 0.25009575486183167\n",
            "emperatriz,: 0.24074670672416687\n",
            "pulsos: 0.23711787164211273\n",
            "zoraida.: 0.23668810725212097\n",
            "trataba,: 0.23599544167518616\n",
            "llenas.: 0.23574165999889374\n",
            "\n",
            "BASE WORD = ayer\n",
            "engañado,: 0.28074684739112854\n",
            "flores,: 0.26050686836242676\n",
            "oyeron,: 0.24891723692417145\n",
            "esas: 0.2394324392080307\n",
            "viniere.: 0.23268912732601166\n",
            "vencerlos: 0.22676023840904236\n",
            "siglos;: 0.22530177235603333\n",
            "suceda: 0.22024933993816376\n",
            "agradecer: 0.2188732773065567\n",
            "merecían: 0.21837635338306427\n"
          ]
        }
      ]
    }
  ]
}